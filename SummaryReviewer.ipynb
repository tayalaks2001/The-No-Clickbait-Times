{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\65835\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors,Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'C:\\Users\\65835\\Desktop\\projects\\googlenewsvectorsnegative300\\GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tokenize the string into words\n",
    "\n",
    "def tokenizing(ss1,ss2):\n",
    "    sentence1 = word_tokenize(ss1)\n",
    "    sentence2 = word_tokenize(ss2)\n",
    "    return sentence1,sentence2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean the data : remove non alphanumeric characters\n",
    "\n",
    "def cleaning(sentence1,sentence2):\n",
    "    cleaned_ss1=[]\n",
    "    for i in sentence1:\n",
    "        if (i not in stopWords) and (i.isalnum()) :\n",
    "            cleaned_ss1.append(i)\n",
    "\n",
    "    cleaned_ss2=[]\n",
    "    for i in sentence2:\n",
    "        if i not in stopWords and i.isalnum():\n",
    "            cleaned_ss2.append(i)\n",
    "    return cleaned_ss1,cleaned_ss2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first function with a user defined formula to check if two sentences are related or similar\n",
    "\n",
    "def check_spam(cleaned_ss1,cleaned_ss2):\n",
    "    count = 0\n",
    "    for i in cleaned_ss1:\n",
    "        for j in cleaned_ss2:\n",
    "            try:\n",
    "                if model.similarity(i,j)>=0.3 :\n",
    "                    count+=1\n",
    "            except:\n",
    "                pass\n",
    "    if count>=(len(cleaned_ss1)*len(cleaned_ss2))**0.5:\n",
    "        spam = False\n",
    "    else :\n",
    "        spam = True\n",
    "    return spam\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the norm(magnitude) of the vector\n",
    "\n",
    "def magnitude(vector):\n",
    "    sum_of_squares = 0\n",
    "    k=0\n",
    "    for num in vector:\n",
    "        sum_of_squares += (num*num)\n",
    "        k+=1\n",
    "    result = sum_of_squares**0.5\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns the dot product of two vectors\n",
    "\n",
    "def dot_product(u,v):\n",
    "    dot = 0\n",
    "    k=0\n",
    "    for num in v:\n",
    "        dot+=u[k]*v[k]\n",
    "        k=k+1\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second function to check if two sentences are similar\n",
    "\n",
    "def double_check(cleaned_ss1,cleaned_ss2):\n",
    "    tolerance = 0.3\n",
    "    v=[0]\n",
    "    for i in cleaned_ss1:\n",
    "        try:\n",
    "            v=v+model[i]\n",
    "        except:\n",
    "            pass\n",
    "    u=[0]\n",
    "    for i in cleaned_ss2:\n",
    "        try:\n",
    "            u=u+model[i]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    if(magnitude(u)==0 or magnitude(v)==0):\n",
    "        spam = True\n",
    "        cosine = 0\n",
    "    else :\n",
    "        cosine=dot_product(u,v)/(magnitude(v)*magnitude(u))\n",
    "        if(cosine>tolerance):\n",
    "            spam = False\n",
    "        else:\n",
    "            spam = True\n",
    "    return spam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third function to check if two sentences are similar or not\n",
    "\n",
    "def triple_check(cleaned_ss1,cleaned_ss2):\n",
    "    tolerance = 1.5\n",
    "    v=[0]\n",
    "    for i in cleaned_ss1:\n",
    "        try:\n",
    "            v=v+model[i]\n",
    "        except:\n",
    "            pass\n",
    "    u=[0]\n",
    "    for i in cleaned_ss2:\n",
    "        try:\n",
    "            u=u+model[i]\n",
    "        except:\n",
    "            pass\n",
    "    diff = magnitude(u)-magnitude(v)\n",
    "    \n",
    "    if(-tolerance<=diff<=tolerance):\n",
    "        spam = False\n",
    "    else:\n",
    "        spam = True\n",
    "    return spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam : False\n",
      "Spam : False\n",
      "Spam : True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# string values of headline and summary must be taken from a column from the main dataframe using a loop(must put whole code in loop after both columns have been extracted)\n",
    "\n",
    "headline = \"ASAP Ferg And Remy Ma Rep The East Coast On New Song \"\n",
    "headline = headline.lower()\n",
    "summary = \"ASAP Fergâ€¯is repping for the East Coa on his new song, and brings fellow NYC rapper Remy Ma along for the ride.\"\n",
    "summary = summary.lower()\n",
    "\n",
    "\n",
    "headline,summary = tokenizing(headline , summary)\n",
    "headline,summary = cleaning(headline , summary)\n",
    "\n",
    "\n",
    "check_1 = check_spam(headline , summary)\n",
    "check_2 = double_check(headline , summary)\n",
    "check_3 = triple_check(headline , summary)\n",
    "\n",
    "# If any 2 out of the 3 check variables are True , then the news article is click-bait.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
